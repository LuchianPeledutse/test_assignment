{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b6b6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoRead(object):\n",
    "    \"\"\"\n",
    "    Class representing frame by frame video reading \n",
    "    \"\"\"\n",
    "    def __init__(self, video_path=None):\n",
    "        self.video_path = video_path\n",
    "        self.height = None\n",
    "        self.width = None\n",
    "        self.fps = None\n",
    "    \n",
    "    def get_frames(self):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Generator function for yielding video frames at each step\n",
    "        \"\"\"\n",
    "        video_capture = cv2.VideoCapture(self.video_path)\n",
    "        #saving image characteristics\n",
    "        self.height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        #creating frames generator\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            #process picture frame\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            tensor_img = torch.from_numpy(rgb_frame).permute(2,0,1).float() / 255.0\n",
    "            #stop yielding when video ends\n",
    "            if not ret:\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                yield tensor_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "006e8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_read = VideoRead('/home/luchian/prog/test_assignment/data/crowd.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84ff4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = vid_read.get_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc461178",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_frame = next(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa065e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff3dba9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5569, 0.4706, 0.2667,  ..., 0.4078, 0.2941, 0.2431],\n",
       "         [0.5176, 0.3686, 0.2235,  ..., 0.4235, 0.3529, 0.3333],\n",
       "         [0.3882, 0.2745, 0.2157,  ..., 0.3686, 0.3922, 0.4314],\n",
       "         ...,\n",
       "         [0.3882, 0.4157, 0.4314,  ..., 0.9765, 1.0000, 1.0000],\n",
       "         [0.3922, 0.4078, 0.4745,  ..., 0.9765, 1.0000, 1.0000],\n",
       "         [0.4941, 0.4980, 0.5725,  ..., 0.9765, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.6000, 0.5137, 0.3098,  ..., 0.4275, 0.3137, 0.2627],\n",
       "         [0.5608, 0.4118, 0.2667,  ..., 0.4431, 0.3725, 0.3529],\n",
       "         [0.4353, 0.3216, 0.2627,  ..., 0.3882, 0.4118, 0.4510],\n",
       "         ...,\n",
       "         [0.3725, 0.4000, 0.4157,  ..., 0.9569, 0.9843, 1.0000],\n",
       "         [0.3765, 0.3922, 0.4588,  ..., 0.9569, 0.9882, 1.0000],\n",
       "         [0.4784, 0.4824, 0.5569,  ..., 0.9569, 0.9922, 0.9922]],\n",
       "\n",
       "        [[0.4353, 0.3490, 0.1451,  ..., 0.3020, 0.1882, 0.1373],\n",
       "         [0.3961, 0.2471, 0.1020,  ..., 0.3176, 0.2471, 0.2275],\n",
       "         [0.2431, 0.1294, 0.0706,  ..., 0.2627, 0.2863, 0.3255],\n",
       "         ...,\n",
       "         [0.3333, 0.3608, 0.3765,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.3373, 0.3529, 0.4196,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.4392, 0.4431, 0.5176,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5f484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f71684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoad(object):\n",
    "    \"\"\"\n",
    "    Class representing detection model with pretrained weights\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.ret = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

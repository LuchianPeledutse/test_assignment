{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6b6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoReader:\n",
    "    \"\"\"\n",
    "    Frame by frame video reading\n",
    "\n",
    "    Reads a video and implements a generator for video frames\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_path : str, default='./crowd.mp4'\n",
    "        System path for video reading\n",
    "\n",
    "    img_resize : int, default=1280\n",
    "        Size to resize image\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    video_capture : cv2.VideoCapture\n",
    "        Object capturing video frames\n",
    "\n",
    "    width : int\n",
    "        Width of video frames\n",
    "    \n",
    "    height : int\n",
    "        Height of video frames\n",
    "    \n",
    "    fps : int\n",
    "        Video fps\n",
    "    \n",
    "    transforms : torchvision.transforms.Resize\n",
    "        Transform for image resizing\n",
    "    \"\"\"\n",
    "    def __init__(self, video_path: str = 'crowd.mp4', img_resize: int = 1088):\n",
    "        self.video_capture = cv2.VideoCapture(video_path)\n",
    "        # Saving image characteristics to use for later video writing\n",
    "        self.height = int(self.video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(self.video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.fps = self.video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        # Applying transformations to fit model input size\n",
    "        self.transforms = transforms.Resize((img_resize, img_resize))\n",
    "    \n",
    "    def generate_frames(self):\n",
    "        \"\"\"\n",
    "        Generator function for yielding video frames at each time step\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        ret : bool\n",
    "            Frame presence flag\n",
    "        \n",
    "        tensor_img : torch.Tensor\n",
    "            Original Numpy image\n",
    "            Tensor image\n",
    "        \"\"\"\n",
    "        # Creating generator to return video frames\n",
    "        while True:\n",
    "            ret, frame = self.video_capture.read()\n",
    "            # Checking if frame is present\n",
    "            if not ret:\n",
    "                break\n",
    "            # Image processing to fit the frame to model input\n",
    "            tensor_img = torch.from_numpy(frame).permute(2, 0, 1)\n",
    "            tensor_img = tensor_img.unsqueeze(dim=0).float() / 255.0\n",
    "            tensor_img = self.transforms(tensor_img)\n",
    "\n",
    "            yield frame, tensor_img\n",
    "    \n",
    "    def release(self) -> None:\n",
    "        \"\"\"Releases the video after it is read\"\"\"\n",
    "        self.video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ba04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoSaver:\n",
    "    \"\"\"\n",
    "    Frame by frame video writing\n",
    "\n",
    "    Writes frames to video with specified width, height, and fps\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fps : float\n",
    "        Video fps\n",
    "\n",
    "    width : int\n",
    "        Video frame width\n",
    "\n",
    "    height : int\n",
    "        Video frame height\n",
    "\n",
    "    path : str\n",
    "        Path to write a video into\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    writer : cv2.VideoWriter\n",
    "        Object for writing videos\n",
    "\n",
    "    fps : float\n",
    "        Video fps\n",
    "\n",
    "    width : int\n",
    "        Video frame width\n",
    "\n",
    "    height : int\n",
    "        Video frame height\n",
    "\n",
    "    save_path : str\n",
    "        Path to write a video into\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, width: int, height: int,\n",
    "                 path: str = 'videoWithBoundingBoxes.mp4'):\n",
    "        self.fps = fps\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.save_path = path\n",
    "        self.writer = cv2.VideoWriter(self.save_path, \n",
    "                                      cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                                      self.fps, (self.width, self.height))\n",
    "    \n",
    "    def add_frame(self, frame: np.ndarray) -> None:\n",
    "        \"\"\"Adds a frame to video\"\"\"\n",
    "        self.writer.write(frame)\n",
    "    \n",
    "    def release(self) -> None:\n",
    "        \"\"\"Releases object resources after the video is written\"\"\"\n",
    "        self.writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53047076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecDrawing:\n",
    "    \"\"\"\n",
    "    Prediction objects drawing\n",
    "\n",
    "    Draws rectangles, labels, and model confidence on video frames\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    picture_size : tuple[int, int]\n",
    "        Video writing image (width, height) for final video frame output\n",
    "    \n",
    "    model_picture_size : tuple[int, int]\n",
    "        Model image (width, height) to scale coordinates\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    picture_size : tuple[int, int]\n",
    "        Image (width, height) to resize final video frame output to \n",
    "    \n",
    "    model_picture_size : tuple[int, int]\n",
    "        Model image (width, height) to scale coordinates\n",
    "    \"\"\"\n",
    "    def __init__(self, picture_size: tuple[int, int], model_picture_size: tuple[int, int]):\n",
    "        self.picture_size = picture_size\n",
    "        self.model_picture_size = model_picture_size\n",
    "    \n",
    "    def draw(self, picture, prediction) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a picture and prediction draws prediction on picture inplace\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        picture : np.ndarray\n",
    "            Array picture in cv2 format (i.e. uint8)\n",
    "\n",
    "        prediction : tuple[tuple[int, int, int, int], str, float]\n",
    "            Model predctions to draw on picture\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        picture : np.ndarray\n",
    "            Array picture ready to be written to RGB video\n",
    "        \"\"\"\n",
    "        scale_x = self.picture_size[0] / self.model_picture_size[0]\n",
    "        scale_y = self.picture_size[1] / self.model_picture_size[1]\n",
    "\n",
    "        label = prediction[1]\n",
    "        confidence = prediction[2]\n",
    "        bounding_box = prediction[0]\n",
    "\n",
    "        pt1 = int(bounding_box[0]*scale_x), int(bounding_box[1]*scale_y)\n",
    "        pt2 = int(bounding_box[2]*scale_x), int(bounding_box[3]*scale_y)\n",
    "\n",
    "        cv2.rectangle(picture, pt1, pt2, (128, 255, 52), 1)\n",
    "        cv2.putText(picture, f\"{label} {confidence}\",\n",
    "                    (int(bounding_box[0]*scale_x), int(bounding_box[1]*scale_y)-5),\n",
    "                    cv2.FONT_ITALIC, 0.75, (15, 15, 97), 2)\n",
    "            \n",
    "        return picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01413e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoad:\n",
    "    \"\"\"\n",
    "    Detection model with pretrained weights\n",
    "\n",
    "    Loads a pretrained object detection model for inference\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    confidence : float\n",
    "        Model makes prediction if its assurance is above this number\n",
    "    \n",
    "    save_img_flag : bool\n",
    "        Flag to save inference image in current directory\n",
    "    \n",
    "    include_classes : list\n",
    "        A list of classes to include in predictions result\n",
    "    \n",
    "    verbose : bool\n",
    "        Flag to print model inference information\n",
    "\n",
    "    imgsz : int\n",
    "        A size to square a frame to before passing to model\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    model : YOLO\n",
    "        Model used for inference\n",
    "    \n",
    "    confidence : float\n",
    "        Model confidence\n",
    "    \n",
    "    save_img_flag : bool\n",
    "        Flag to save inference image in current directory\n",
    "\n",
    "    verbose : bool\n",
    "        Flag to print model inference information\n",
    "    \n",
    "    include_classes : list\n",
    "        A list of classes to include in predictions result\n",
    "\n",
    "    imgsz : int\n",
    "        A size to square a frame to before passing to model\n",
    "    \"\"\"\n",
    "    def __init__(self, confidence: float = 0.5, save_img_flag: bool = False,\n",
    "                 include_classes: list = ['person'], verbose: bool = False, imgsz=640):\n",
    "        self.model = YOLO(\"yolo11l.pt\")\n",
    "        self.confidence = confidence\n",
    "        self.save_img_flag = save_img_flag\n",
    "        self.include_classes = include_classes\n",
    "        self.verbose = verbose\n",
    "        self.imgsz = imgsz\n",
    "    \n",
    "    def predict(self, picture: torch.Tensor) -> tuple[\n",
    "        np.ndarray,\n",
    "        list[tuple[tuple[int, int, int, int], str, float]]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Makes object predictions based on picture\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        picture : torch.Tensor\n",
    "            Processed tensor picture for model input\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        return_picture : np.ndarray\n",
    "            Numpy array picture ready to be written to another video\n",
    "        \n",
    "        detected_objects : list[tuple[tuple[int, int, int, int], str, float]]\n",
    "            Objects included in the classes list and found on picture \n",
    "        \"\"\"\n",
    "        prediction = self.model(picture, \n",
    "                                save=self.save_img_flag, \n",
    "                                conf=self.confidence, \n",
    "                                verbose=self.verbose,\n",
    "                                imgsz=self.imgsz)\n",
    "        # Peparing variables to store data\n",
    "        detected_objects = []\n",
    "\n",
    "        for box in prediction[0].boxes:\n",
    "            conf = float(box.conf[0])\n",
    "            x1, y1, x2, y2 = box.xyxy[0].to(dtype=torch.int).tolist()\n",
    "            label = self.model.names[int(box.cls[0])]\n",
    "\n",
    "            # Adding only predictions included in list classes to exclude others\n",
    "            if label in self.include_classes:\n",
    "                detection_tuple = ((x1, y1, x2, y2), label, round(conf, 2))\n",
    "                detected_objects.append(detection_tuple)\n",
    "\n",
    "        return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1dddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video() -> None:\n",
    "    \"\"\"\n",
    "    Integrates all the components of a program. \n",
    "    Creates all the steps: model, reading video, drawing, writing video.\n",
    "    Saves results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    model = ModelLoad(imgsz=640)\n",
    "    videoReader = VideoReader(video_path='crowd.mp4', img_resize=640)\n",
    "    videoSaver = VideoSaver(videoReader.fps, videoReader.width, videoReader.height)\n",
    "    imgDraw = ImageRecDrawing(picture_size=(videoReader.width, videoReader.height), model_picture_size=(640, 640))\n",
    "    video_frames = videoReader.generate_frames()\n",
    "    \n",
    "    for numpy_frame, tensor_frame in tqdm(video_frames, desc='Writing the video'):\n",
    "        detected_objects = model.predict(tensor_frame)\n",
    "        \n",
    "        for obj in detected_objects:\n",
    "            numpy_frame = imgDraw.draw(numpy_frame, obj)\n",
    "        \n",
    "        videoSaver.add_frame(numpy_frame)\n",
    "    \n",
    "\n",
    "    videoReader.release()\n",
    "    videoSaver.release()\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da02d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing the video: 705it [06:11,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "make_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
